# 机器学习纳米学位毕业项目
## 新闻组文档分类
Ling Jin 优达学城

2017年9月6日




## I. 文档分类

### 项目概述

随着互联网的蓬勃发展，有不计其数的数据被产生，而文本数据自互联网诞生以来，就占了数据资料中的一大块。在社会领域，每天都有新闻报道；在科研领域，学术文献和科研记录都是非常宝贵的文本资料。现如今，社交网络欣欣向荣，UGC(用户生成内容) 占据了很重要的研究地位。所以，自然语言处理技术在这个文本爆炸的时代显得越发重要了。

文档分类是自然语言处理技术和模式识别密切结合的研究课题，而其也是其他自然语言处理应用的基础。文档分类是在预定义的分类体系下，根据文本的特征，将给定文本与一个或多个类别相关联的过程。对于文档分类而言，其输入是需要进行分类处理的文档，输出则是与文本关联的类别。其过程可以被理解成这样一个函数$\Phi: D \times C \rightarrow \lbrace T, F \rbrace$ 

在本项目中，我使用的数据集是20Newsgroup(http://qwone.com/~jason/20Newsgroups/)这个数据集，该数据集包含了大概20个类别将近20000个新闻组文件。该数据集被广泛地使用在文本分类任务当中。除此之外，文档分类任务被认为是其他高阶任务的一个重要基础，比如在情感计算和舆情分析任务当中，文档分类具有重要的作用。

### 问题陈述

对数据集进行分类，是一个监督学习任务。我的主要工作是构建机器学习模型，对文档数据集进行分类，力图提高文档分类的准确性，保证大多数的文本被正确地分类，并且训练所得到的模型有足够的泛化能力，在未见到过的文档数据上有好的表现。本次项目，使用由CMU维护的20Newsgroup数据集，该数据集中的新闻组文档覆盖了20个不同的主题，其中一些主题之间在更高的层次上有一定的联系。比如graphics，ms-windows 和 sys.mac都可以被认为是关于计算机的主题，hockey和baseball都可以被认为是关于体育运动的主题。在这样的分类粒度下，20个主题可以被粗略地分成6个大主题，我将从中选出6个主题进行本次分类任务，其分别是："comp.os.ms-windows.misc",  "rec.sport.baseball", "sci.space","alt.atheism","talk.politics.mideast"。其分别对应于上文提及到的6个大主题中的一个。这样除了可以削减模型大小，节约内存和计算消耗，还因为类别之间相似程度减小从而提高模型准确率。从通过完成该项目，贯通学习机器学习知识这一目的而言，我认为这样做是合理的。

该数据集有多种导入方法，在此处我将从Sci-kit Learn 中导入该数据集。首先我会研究数据集的分布情况，然后我会查看个别文档，对每个数据样本有个大概的了解。通过对这些文本数据进行预处理，去除多余的噪声， 从中提取出有用的特征信息，并以此为依据构建模型，提出模型评价指标，以指标为依据评价模型好坏，从而选择出性能最好的模型。



### 评价指标
文档分类本质上是寻找一个分类模型，所以该任务中使用的模型评价指标应该是针对分类模型的。在此处，我将会使用精确率(Accuracy)和F1分数这两种评价指标对训练所得的模型进行评价。

Accuracy指的是，在一个分类任务中，如果存在$N$个样本，有$c$个样本被正确地识别分类，那么有如下公式定义：
$$
Accuracy = \frac{c}{N}
$$
该指标最直观地表明了有多大比例的文档被正确地分类了，是一个非常直观切有效的指标。

对于分类任务而言，我们有时候还会考虑这样一个问题，每一个样本被识别分配到了一个类别$C$，那么在这个$C$类中，如果有$m$个被分配样本，这$m$个样本中有$TP$个其本身就是$C$类别，那么就有$FP = m-TP$ 个属于其他类别的样本是被错误分类的。 这个时候，可以得到一个叫做查准率的指标，其数学定义如下：
$$
Precision = \frac{TP}{TP+FP}
$$
其度量的是一个模型的准确性。同样的对于类别$C$而言，如果本属于该类别的$X$个样本中，有$TP$个样本被正确地分类，那么就有$FN = X - TP$个样本是被分配到了错误的类，这里可以得到一个叫做查全率的指标，其数学定义如下：
$$
Recall = \frac{TP}{TP + FN}
$$
该指标度量了一个模型，对于类别$C_i$而言，有多少本属于$C_i$的样本被分到了正确的类，表现的是一个模型的全面性。此处对于查准率和查全率的解释都是简化成了二分类问题而言的，此处可以被泛化为多分类问题。

查准率和查全率本身是一对矛盾的量，一方高了，另外一方必然是高不了的。在本次任务中，我将使用的是F1分数，其定义如下：
$$
\frac{1}{F_1} = \frac {1}{2}(\frac{1}{Precision}+\frac{1}{Recall})
$$
该指标是基于查准率和查全率的调和平均，其权衡了查准率和查全率对模型的评价，保证这一对矛盾的量之间存在一定的平衡。

本次实验是一个多分类问题，也可以被理解成是多个二分类问题。那么就存在$macro\_F_1$和$micro\_F_1$两种计算方式，在此处我们使用的是$macro\_F_1$计算方法。关于这两种计算方法的公式，此处不再赘述。

我们将使用Accuracy和F1分数同时作为评价指标，并且以Accuracy作为主要参考评价指标，因为我们主要关注的是模型是否能够正确将每一个样本分到正确的类别当中，Accuracy更好地表征了这一性能。



## II. 分析
### 数据的探索
本任务所使用的20Newsgroup数据集有多种导入方式，此次使用的是通过 Sklearn自带的数据集的方式载入。数据集中的每一个样本文件如下形式:

> From: andy@ice.stx.com (Andy Moore)
> Subject: Q: How to avoid XOpenDisplay hang?
> Keywords: Xlib
> Reply-To: andy@ice.stx.com
> Organization: Hughes STX, Lanham, MD
> Lines: 13
>
> I'm writing 'xwall', a simple X version of 'wall', and I want it to
> put a message up on each of a default list of displays.  The problem
> is that XOpenDisplay hangs if one of the displays is currently
> controlled by xdm (login screen).   I've been through the manuals
> and FAQ and don't see a simple way to see if a display is 'openable'
> ahead of time, or to get XOpenDisplay to fail after a short period
> of time.  Any hints, suggestions, clues, or pointers to info?  Thanks...
>
> -- 
> Andy Moore (andy@ice.stx.com)
> _______________________________________________________________________________
> "You could say I've lost faith in the politicians/
>  They all seem like game show hosts to me..."          - Sting

几乎所有的文件都是以这样的格式被保存在数据集中。可以明显地区分开主要的内容和其他的附加内容。

在载入数据集的时候，通过设置参数的方式，仅导入在上文中提及到的将会被用到的6个类别。查看文档得知，该数据集中的样本都被分成了包含"headers", "footers", "quotes"这3个特殊部分的形式，在此我不并不打算去掉这些附属部分，因为我假设认为同类的文档集中，这些部分存在相似性，能够对分类任务带来帮助。数据集已经被预先分为了训练集和测试集两个部分。训练集有训练样本3410个，测试集有待测样本2270个。

### 探索性可视化

至此阶段，对于文档样本的预处理工作初步完成。通过可视化手段，查看数据集中各个类别的分布情况。训练集类别分布如下：

![train](c:\workspace\mlnd\project\Graduation\train_without_pre.png)



测试集类别分布如下：

![test](c:\workspace\mlnd\project\Graduation\test_without_pre.png)



训练集和测试集类别分布相似，且样本类别之间不存在特别严重的偏倚，虽然某一类的数量相比其他类而言较少，但是我认为并不会影响模型训练。

### 算法和技术
文本数据有两大特点，其一是单条文本长度不固定，文本之间长度变化大。其二是某个单词在单条句子中反复出现。针对这一问题，tf-idf是一项非常有用的技术，如果某个单词在某个文档中出现频率很高，那么可以认为这个单词对该文档很重要，可如果这个单词在其他文档中出现的频率也很高，那么其重要程度可能并没有想象的那么高，比如说一些常用的形容词，助词，像"like"之类的词，这种词往往对分类问题没有什么帮助。某些特定领域的词，例如“windows”，“card","hitter"等专业术语多出现在某一类文档中，对分类问题有一定帮助。

文本数据因为单词量太多的缘故，维度通常会特别高，一般的算法有时很难处理维度过高的数据。朴素贝叶斯算法和支持向量机模型长期以来在文本分类等相关任务上取得了非常理想的效果。

朴素贝叶斯算法非常适合多分类任务，其通过对数据集进行概率计算，生成一个判别式模型，在小规模的数据集上也有非常好的表现。本次任务的训练集和测试集都只有3000左右的样例，贝叶斯算法应该是能够在这个数据集上有好的表现。但其缺点在于，其理论方法是基于假定各个特征之间条件独立的，这一假定并不总是成立，在某些情况下性能不一定好。朴素贝叶斯算法理论简单，容易理解，而且以往战绩非常好，值得一用。

支持向量机模型在分类问题上一直表现突出，无论是而分类还是多分类，线性或者非线性，都有非常好的表现，其泛化错误率较低，并且计算开销不大，通过在高位空间中寻找超平面分割数据集的方法，其原理在直觉上容易被人接受。在数据集较少，而且数据维度较高的情况下，有很好的表现。但是该模型往往和核方法的选择有很大的关系，需要进行大量的调参工作，以达到一个相对最优的结果，而绝对的最优是很难的。

### 基准模型
初步预定，假设在该数据集上训练所得到的模型能够在训练集上尽可能百分之百正确，且在测试集上的表现尽可能好，因为存在模型拟合不好控制的问题，我希望将偏差控制在8%到10%以内，也就是在测试集上能够获得90%以上的准确率和F1分数。这表明模型已经对绝大多数的数据样本起了作用，能够分类绝大多数的文档，我认为这是要给可以被接受的基准指标。



## III. 方法
### 数据预处理
文本资料的预处理有一套较为规范的流程。在该项目中，我将按照以下流程进行操作：

1. 查看其中个别文档实例，分析其中的用词情况，主要是单词的大小写，是否存在很多转义字符的情况，是否存在很多特有领域专有名字缩写的情况，对文档具体内容有个初步的感性认知。
2. 基于步骤一，对每一个文本进行Tokenization，即将字符串切分为一个个单独的单词，并且去除多余的空白格，仅保留英文单词在单独的列表当中。
3. 在文本挖掘领域中，stopwords是一个非常常见的概念。在不同的语言中，有一些被频繁使用的词，多为助动词或者代词之类的，比如 I, you, be, are, this, that, etc. 这些词在每一篇文章中都广泛存在，对于文本分类任务没有帮助，在此处需要将其移除，从而降低特征维度。
4. 再次查看经过前三步处理之后的文档内容，发现其中由一些长度为3的字母组合，按照个人推断，我认为这些字母组合很有可能是专有领域名词缩写，所以保留这些单词，不做特殊处理。但是存在一些单独的字母和长度为2的字母组合，我认为这些是在第二步移除特殊字符的时候被引入的错误，在此将其视作噪声，对其移除。

预处理完成之后，查看训练集和测试集样本的单词列表大小。

| 样本集  | 最长   | 最短   | 平均   |
| :--- | ---- | ---- | ---- |
| 训练集  | 4758 | 13   | 165  |
| 测试集  | 6188 | 12   | 164  |

由此可知，从样本中的单词个数来讲，训练集和测试集的差别也不大。具体样本单词量分布如下：

![train_word_count](C:\workspace\mlnd\project\Graduation\train_word_count.png)

![test_word_count](C:\workspace\mlnd\project\Graduation\test_word_count.png)

由上图可知，训练集和测试集样本长度的分布类似。

### 执行过程

首先选择朴素贝叶斯模型，在Sklearn这个库中，提供了3种贝叶斯模型，在本次训练中使用的是MultinomialNB这个模型，因为其数学原理计算过程中会考虑特征频率问题，非常适合具有离散特征的分类问题，所以该模型长期被应用在文本分类任务中。

然后使用支持向量机模型拟合数据。LinearSVC 和 SVC是两个可选的模型，一开始的时候，我尝试使用SVC模型进行数据拟合，但是运行异常缓慢，之后尝试使用LinearSVC发现训练速度非常快，且其表现在训练集和测试集上都非常好。通过查看文档得知，SVC基于libsvm实现，默认使用的是’rbf'核方法，因为其训练拟合数据所需要的时间随数据集大小呈现比平方级还要高的增长，所以很难用于数据量很多的样本或样本维度很高的情况。而LinearSVC底层则是基于liblinear，可以被认为是SVC使用linear核方法的特殊版本，在多样本数据集的情况下，表现得更好。

在实际操作时，我借用了之前作业里的代码片段，将模型训练，模型预测，指标评价封装到了一起，更方面进行实验对比。

实验过程如下：

1. 使用TfidfVectorizer方法获取tfidf的值矩阵。在训练集上使用fit_transform方法对训练集进行拟合，提取出训练集的特征值。并且使用在训练集上拟合所得到的模型应用transform方法对测试集进行变换，获取测试集的特征值。
2. 将第一步中获取得到的训练集和测试集特征矩阵分别传入模型中进行训练，所得模型分别记为$m1$和$m2$

| Model         | train time | train accuracy | train f1 | test accuracy | test f1 |
| ------------- | ---------- | -------------- | -------- | ------------- | ------- |
| MultinomialNB | 0.5640     | 0.9947         | 0.9947   | 0.9520        | 0.9515  |
| LinearSVC     | 0.4870     | 1.0000         | 1.0000   | 0.9542        | 0.9536  |

由上表格可以得出总结，这两个模型都能够非常快地训练模型，并且在训练集上的效果近乎于100%。通过对比训练集和测试机上的表现，发现还存在4%左右的偏差，初步认为这是过拟合现象，解决这一问题的方法可以是通过网格搜索法，对超参数进行调整，试图寻找到一组表现最好的参数，减轻过拟合问题，试图令模型很好地拟合且在测试集上表现得更为优秀。也可以通过增大数据集的方式，让模型见到更多的样本点，从而减弱过拟合现象。

### 完善

通过使用网格搜索方法，划定一个超参数搜索空间，试图寻求对模型性能有所提升的超参数组合。部分重要代码如下：

```python
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, fbeta_score

parameters = {"C": [1, 1.5, 2],
              "loss": ["hinge", "squared_hinge"],
              "tol":[0.1, 0.5]
              }

linear_svc = LinearSVC(random_state=422)
f1_scorer = make_scorer(fbeta_score, beta=1, pos_label='yes', average='macro')

grid_obj = GridSearchCV(linear_svc, parameters, f1_scorer)
grid_obj.fit(train_tfidf, train_y)

# Get the estimator
clf = grid_obj.best_estimator_
```

最后的到了一个模型，将其记作$m3$，其在测试集上的F1_score为0.9555， Accurracy soce为0.9559。相比之前的模型，提升非常小。查看模型参数如下：

> ```
> {'C': 2,
>  'class_weight': None,
>  'dual': True,
>  'fit_intercept': True,
>  'intercept_scaling': 1,
>  'loss': 'squared_hinge',
>  'max_iter': 1000,
>  'multi_class': 'ovr',
>  'penalty': 'l2',
>  'random_state': 422,
>  'tol': 0.5,
>  'verbose': 0}
> ```

经过超参数空间搜索得到的新模型的评价指标得分高于默认参数模型得分，该模型$m3$当前可以被认为是最优模型。

#### 特征提取之卡方检验

卡方验证方法是通过衡量某个特征量$t_i$和类别$C_j$之间的相关联程度，并假设二者之间符合具有一阶自由度的卡方分布。特征对某个类别的卡方统计值越高，二者之间的相关性越大，携带的信息也就较多，对分类就更有帮助。

在此处，使用卡方检验方法选择携带信息最多的K个特征项，试图使用这K个特征项提升模型表现。

令K的取值范围为$[1000, 10000]$，间隔为500，使用sklearn自带的特征选择方法，分别提取出相应的K个特征项，作为模型输入，再分别将其使用到LinearSVC模型和MultinomialNB模型上，记录其准确率和F1分数随着K值增大的变化情况。在MultinomialNB模型的表现如下：

![multinomialNB_chi_acc](C:\workspace\mlnd\project\Graduation\multinomialNB_chi_acc.png)

![multinomialNB_chi_f1](C:\workspace\mlnd\project\Graduation\multinomialNB_chi_f1.png)

在LinearSVC模型上的表现如下：

![LinearSVC_accuracy](C:\workspace\mlnd\project\Graduation\LinearSVC_accuracy.png)

![LinearSVC_f1](C:\workspace\mlnd\project\Graduation\LinearSVC_f1.png)

分析模型在测试集上的Accuracy得分情况，试图寻找出特征为度和所的模型性能之间是否存在某些联系。上述4图的曲线效果走势类似，说明随着特征数量的提高，对分类的模型性能提升有正向帮助。

对于MultinomialNB模型，当特征维度为7000的时候，模型能够在测试集上取得最大的Accuracy。以此为依据创建$m4$模型，其在测试集上的Accuracy和F1分数分别为0.9577和0.9574。

对于LinearSVC模型，当特征为度为10000的时候，模型能够在测试集上取得最大的Accuracy。以此为依据创$m5$模型，其在测试集上的Accuracy和F1分数分别为0.9559和0.9552。



## IV. 结果

### 模型的评价与验证
上述实验过程中，首先使用TFIDF特征矩阵作为训练模型的输入，初步获得了一个可以被接受的结果，并且试图使用网格所搜的方法寻找到一组超参数，令模型在性能上取得进一步的提升。然后，又使用了卡方检验方法搜寻前K个对分类最有帮助的特征作为训练输入，训练所得到的模型效果也是非常可观的。

在此处，我选取$m4$模型作为最终的模型，其具体是一个经过了超参数搜索而来的线性支持向量机，因为在所有获得的模型当中，它在测试数据集上的综合表现是最好的，以此为依据也认为它是最好的模型。该模型达到了之前所设定的评价指标基准，可以被认为与之前所期待的结果是一致的。查看具体参数，与模型默认训练超参数对比，参数'tol'被设置为0.5，损失函数被设置为'hinge loss'，其他超参数和默认值相同。'tol'参数代表的是停止训练的误差大小，在该模型中设置为0.5，增大了模型对误差的容忍度，可以被理解为是通过允许在训练阶段容忍个别错误，以试图提升在测试集上的表现。之前查看训练集中的数据内容，发现其中存在着许多噪声单词，在预处理过程中，并未剔除这部分的单词，保留其在训练集中，所以我认为在其上训练而得到的模型对于输入具有一定的鲁棒性，能够容忍存在这噪声的输入。

### 合理性分析
最佳模型在测试集上，准确率为0.9398，F1分数为0.9396，这两个评价质保都高于在之前设定的基准模型指标，并且模型在其训练集上的表现有准确率为0.9974和F1分数0.9974，测试集上的表现不及在训练集上的表现，但是偏差值在0.6左右，存在偏差，但是偏差范围较小，可以被接受。在测试集上的指标，保证了该模型能够分类绝大多数的未知文档，所以我认为该模型可以被用于解决文档分类问题。



## V. 项目结论

### 结果可视化
在这一部分，你需要用可视化的方式展示项目中需要强调的重要技术特性。至于什么形式，你可以自由把握，但需要表达出一个关于这个项目重要的结论和特点，并对此作出讨论。一些需要考虑的：

你是否对一个与问题，数据集，输入数据，或结果相关的，重要的技术特性进行了可视化？_

可视化结果是否详尽的分析讨论了？_

绘图的坐标轴，标题，基准面是不是清晰定义了？_

### 对项目的思考
在这一部分，你需要从头到尾总结一下整个问题的解决方案，讨论其中你认为有趣或困难的地方。从整体来反思一下整个项目，确保自己对整个流程是明确掌握的。需要考虑：
- _你是否详尽总结了项目的整个流程？_
- _项目里有哪些比较有意思的地方？_
- _项目里有哪些比较困难的地方？_
- _最终模型和结果是否符合你对这个问题的期望？它可以在通用的场景下解决这些类型的问题吗？_


### 需要作出的改进
在这一部分，你需要讨论你可以怎么样去完善你执行流程中的某一方面。例如考虑一下你的操作的方法是否可以进一步推广，泛化，有没有需要作出变更的地方。你并不需要确实作出这些改进，不过你应能够讨论这些改进可能对结果的影响，并与现有结果进行比较。一些需要考虑的问题：
- _是否可以有算法和技术层面的进一步的完善？_
- _是否有一些你了解到，但是你还没能够实践的算法和技术？_
- _如果将你最终模型作为新的基准，你认为还能有更好的解决方案吗？_

----------
** 在提交之前， 问一下自己... **

- 你所写的项目报告结构对比于这个模板而言足够清晰了没有？
- 每一个部分（尤其**分析**和**方法**）是否清晰，简洁，明了？有没有存在歧义的术语和用语需要进一步说明的？
- 你的目标读者是不是能够明白你的分析，方法和结果？
- 报告里面是否有语法错误或拼写错误？
- 报告里提到的一些外部资料及来源是不是都正确引述或引用了？
- 代码可读性是否良好？必要的注释是否加上了？
- 代码是否可以顺利运行并重现跟报告相似的结果？